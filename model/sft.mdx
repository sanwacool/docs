---
title: "SFT/DPO/RL"
---

太好了，这一步其实**非常关键**：\
你现在不是在“听懂别人说的话”，而是在**把一整套 RL / SFT / 推理 / 系统工程的认知沉淀成自己的知识结构**。

下面我会把**今天我们讨论的所有点**系统性地整理成一篇**可以长期保存、反复回看的学习资料**。\
我会尽量做到：

- 结构清晰（可以当笔记）
- 概念分层（直觉 → 技术 → 工程）
- 把你和算法工程师的对话**完整地解释清楚**

---

# 一、背景：我们在讨论什么问题？

我们讨论的核心不是“某个算法细节”，而是这个大问题：

> **在大模型训练中，SFT / DPO / RL 分别在解决什么问题？\
> 为什么工业界现在大量用 SFT / DPO，而真正的 RL 推进很慢？**

你提出的关键直觉是：

> “SFT 学的是最好的答案，而 RL 只是从自己生成的答案里选一个更好的，\
> RL 看起来更像‘稳定性提升’，而不是‘上限提升’。”

这是一个**非常专业、而且是对的起点**。

---

# 二、三种训练范式的本质差异

## 1️⃣ SFT（Supervised Fine-Tuning）

### 1.1 一句话定义

> **SFT = 用“已知的好答案”直接教模型该怎么回答**

---

### 1.2 SFT 在学什么？

数学上不展开，你只记住一件事：

> **SFT 的目标是：\
> 给定 prompt，最大化“这个 token 序列的概率”**

所以它在做的是：

- 模仿
- 复现
- 逼近老师分布

---

### 1.3 用更强模型做 SFT，为什么有效？

你说的这点是完全正确的：

> “我用 Gemini / GPT-4 生成数据，再 SFT 自己的模型，效果确实很好。”

原因是：

- 上限来自 **老师模型**
- 学生模型在逼近老师的输出分布

但也因此：

> **SFT 的理论上限 ≤ 老师模型**

---

### 1.4 SFT 的优缺点总结

**优点**

- 稳定
- 工程简单
- 效果立竿见影

**缺点**

- 上限受限
- 容易“学会表面答案”
- 很难学到“过程正确但答案多样”的能力（尤其是代码）

---

## 2️⃣ DPO（Direct Preference Optimization）

### 2.1 一句话定义

> **DPO = 不学“什么是最好”，而是学“哪个更好”**

---

### 2.2 DPO 在优化什么？

DPO 的核心不是 reward，而是一个偏好对：

```
(prompt, chosen_response, rejected_response)
```

模型学的是：

> “在同一个 prompt 下，我更应该偏向 chosen，而不是 rejected。”

---

### 2.3 DPO 带来的典型效果

你说的这句话非常准确：

> “DPO 之后，模型表现会更稳定。”

原因是：

- 压低坏回答的概率
- 收敛到“安全、稳妥、评测友好”的区域

但要注意：

> **稳定 ≠ 一定突破上限**

---

### 2.4 为什么算法工程师说“这不一定”？

因为：

- DPO 的效果**高度依赖数据**
- 你现在看到的提升，可能来自：
  - 数据分布
  - 数据质量
  - 超参数\
    而不一定是 DPO 算法本身

这就引出了下面两个关键概念。

---

# 三、数据消融 & 训练超参

## 3️⃣ 数据消融（Data Ablation）

### 3.1 一句话理解

> **数据消融 = 刻意去掉一部分数据，看模型还会不会好**

---

### 3.2 为什么必须做数据消融？

因为在工业场景中：

> **“模型变好了” ≠ “算法是对的”**

可能是：

- Gemini 数据多了
- 人工数据质量高
- 某一类任务刚好命中评测

---

### 3.3 消融回答的问题是

> “**到底是算法起作用，还是数据在‘托底’？**”

---

## 4️⃣ 训练超参（Hyperparameters）

### 4.1 一句话理解

> **训练超参 = 你事先设定的、模型自己学不会的参数**

例如：

- learning rate
- KL 系数
- PPO clip
- batch size

---

### 4.2 为什么算法工程师强调“还在调”？

因为在 RL / DPO 中：

> **超参对结果的影响，可能比算法本身还大**

所以在没有完整 sweep 前：

- 不敢下结论
- 不敢说“DPO 一定更好”

---

# 四、RL：为什么它理论上可以提升上限？

这是你提的**最关键的问题**。

---

## 5️⃣ RL 和 SFT 的根本差异

### SFT 的世界观

> “**什么样的答案是正确的？**”

### RL 的世界观

> “**什么样的结果是好的？**”

---

## 6️⃣ 为什么 RL 在代码任务上能突破上限？

这是核心结论：

### 6.1 代码任务的特殊性

代码任务往往：

- 解法不唯一
- 中间过程很重要
- 正确性可以被程序验证（compile / test）

---

### 6.2 RL 能优化 SFT 优化不了的东西

例如：

- 是否通过测试
- 是否在复杂路径下也正确
- 是否学会“尝试 → 修正 → 再尝试”

这类能力：

> **很难从“单条标准答案”中学到**

---

### 6.3 所以你的直觉需要修正一点点

你说：

> “RL 更像是稳定性，不提升上限。”

更准确的说法是：

> **RL 的上限提升依赖于：\
> reward 是否能定义出‘老师模型也没覆盖的好行为’**

代码任务刚好满足这一点。

---

# 五、在线 RL vs 离线 RL

## 7️⃣ 离线 RL（Offline RL）

### 一句话

> **在已有数据上做 RL 更新**

数据来源可以是：

- Gemini
- GPT-4
- 历史 rollout

本质上：

- 还是离不开强模型

---

## 8️⃣ 在线 RL（Online RL）

### 一句话

> **模型自己生成 → 自己评估 → 自己更新**

这是你问的：

> “是不是就不需要 Gemini 了？”

**是的，但前提是你具备完整 RL infra。**

---

# 六、为什么你们现在“不具备在线 RL 条件”？

这是工程现实。

---

## 9️⃣ RL Infra 是什么？

### 一句话定义

> **RL infra = 支撑在线 RL 运转的一整套系统**

---

### 至少包括：

1. **Rollout 引擎**
   - 高并发推理（vLLM）
2. **Reward 系统**
   - Reward Model
   - 规则 / 执行器
3. **训练系统**
   - PPO / policy update
   - Megatron / FSDP
4. **数据管道**
   - trajectory
   - advantage
   - buffer
5. **调度 & 同步**
   - rollout ↔ trainer

---

## 🔟 为什么 seed 模型卡住了？

因为 seed 模型是：

- 闭源
- 定制结构
- 私有算子

这导致：

> **无法直接接入开源 RL 框架（TRL / vLLM / Megatron）**

要做在线 RL，意味着：

- 在开源框架下**重新实现一整套模型支持**
- 成本极高

---

# 七、Megatron、推理引擎、训练引擎

## 1️⃣1️⃣ Megatron 是什么？

> **一个让超大模型能被多卡训练的系统**

负责：

- 张量并行
- 流水并行
- 高效反向传播

---

## 1️⃣2️⃣ 推理引擎 vs 训练引擎

| 维度 | 推理引擎              | 训练引擎          |
| :-- | :---------------- | :------------ |
| 目标 | 快速生成              | 更新参数          |
| 核心 | decode / KV cache | backward / 梯度 |
| 代表 | vLLM              | Megatron      |

---

## 1️⃣3️⃣ RL 为什么最难？

因为：

> **RL = 推理系统 + 训练系统 + 数据闭环**

---

# 八、最终总结（给你一个“定级结论”）

你今天已经理解到这个层级：

> **不是“某个算法好不好”，\
> 而是“在当前组织、模型、infra 约束下，什么是最现实、性价比最高的路线”。**

---

### 在你们的现实条件下：

- **短期**
  - SFT + DPO + 离线 RL
- **中期**
  - 强化 reward model
- **长期**
  - 如果 seed 模型可接入 → 在线 RL

---